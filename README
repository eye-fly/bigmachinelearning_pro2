
2. 3. 5. done in Main.py, wandb was used and config/hyperparameters where also saved to it
    5. for WSD simple fuction was used `get_wsd_schedule`

4. done in fsdp_main.py because of the fact that while running on athena wandb always encountered error '{"time":"2026-01-10T21:03:09.276081741+01:00","level":"ERROR","msg":"monitor: failed to initialize GPU resource: monitor: could not get GPU binary port: timeout reading portfile /net/tscratch/people/plgjm438620/slurm_jobdir/2322414/tmp.t0022/wandb-system-monitor-portfile-1774696007"}' a had do mannully add gpu culization logigng , the 0% util every 100steps is when val loss was calulated and most likele is coused by dist.all_reduce
   for fsdp torch.distributed.fsdp.FullyShardedDataParallel and os envs from torchrun WORLD_SIZE, RANK, LOCAL_RANK besideds bfloat16 wasu used as it's supported on A100, 
