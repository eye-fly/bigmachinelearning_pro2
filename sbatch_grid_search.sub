#!/bin/bash
#SBATCH --job-name=chinchilla_grid
#SBATCH --time=01:00:00
#SBATCH --partition=plgrid-gpu-a100
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=4
#SBATCH --mem=32G
#SBATCH --array=0-2
#SBATCH --output=logs/grid_%A_%a.txt
#SBATCH --error=logs/grid_%A_%a.txt

mkdir -p logs
source .venv/bin/activate

LRS=(1e-2 1e-3 1e-4)
LR=${LRS[$SLURM_ARRAY_TASK_ID]}

echo "Starting Grid Search Task ID: $SLURM_ARRAY_TASK_ID"
echo "Selected Learning Rate: $LR"
echo "Nodes: $SLURM_JOB_NODELIST"

MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR
export MASTER_PORT=29500

echo $WORLD_SIZE
export WORLD_SIZE=$(($SLURM_JOB_NUM_NODES * 1))

export NCCL_SOCKET_IFNAME=ib0
export GLOO_SOCKET_IFNAME=svc-eth
# export NCCL_DEBUG=INFO

echo "MASTER_ADDR=$MASTER_ADDR"
echo "NCCL_SOCKET_IFNAME=$NCCL_SOCKET_IFNAME"
echo "GLOO_SOCKET_IFNAME=$GLOO_SOCKET_IFNAME"
echo "WORLD_SIZE=$WORLD_SIZE"

srun torchrun \
    --nnodes=$SLURM_JOB_NUM_NODES \
    --nproc_per_node=1 \
    --rdzv_id=$SLURM_JOB_ID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
    grid_search.py \
    --learning_rate $LR \
    --batch_size 256 \
    --n_layers 4 \
    --dmodel 256 \
    --n_heads 4

# 